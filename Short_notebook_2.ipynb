{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T17:01:52.736200Z",
     "start_time": "2024-11-10T17:01:52.720527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Authors:[146] The Italian Butei\n",
    "\n",
    "# Amadori Luca, ID: 133429, lucaam@stud.ntnu.no\n",
    "# Coppola Rodolfo Emanuele, ID: 133173, rodolfoc@stud.ntnu.no\n",
    "# Meschieri Andrea, ID: 133527, andremes@stud.ntnu.no"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T17:01:54.667333Z",
     "start_time": "2024-11-10T17:01:52.771649Z"
    }
   },
   "cell_type": "code",
   "source": "pip install -r requirements.txt",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: pandas==2.2.1 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn==1.4.2 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from -r requirements.txt (line 3)) (1.4.2)\n",
      "Requirement already satisfied: geopandas==1.0.1 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from -r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from pandas==2.2.1->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from pandas==2.2.1->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from pandas==2.2.1->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from scikit-learn==1.4.2->-r requirements.txt (line 3)) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from scikit-learn==1.4.2->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from scikit-learn==1.4.2->-r requirements.txt (line 3)) (3.4.0)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from geopandas==1.0.1->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from geopandas==1.0.1->-r requirements.txt (line 4)) (24.0)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from geopandas==1.0.1->-r requirements.txt (line 4)) (3.7.0)\n",
      "Requirement already satisfied: shapely>=2.0.0 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from geopandas==1.0.1->-r requirements.txt (line 4)) (2.0.6)\n",
      "Requirement already satisfied: certifi in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from pyogrio>=0.7.2->geopandas==1.0.1->-r requirements.txt (line 4)) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lucaa\\documents\\università\\python\\eplf\\venveplf\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.1->-r requirements.txt (line 2)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\lucaa\\Documents\\Università\\Python\\EPLF\\venvEPLF\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "scYM4Xpn81TE",
    "ExecuteTime": {
     "end_time": "2024-11-10T17:01:57.254071Z",
     "start_time": "2024-11-10T17:01:54.667333Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import geopandas as gpd\n",
    "from shapely.ops import nearest_points\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "# FUNCTIONS:\n",
    "\n",
    "# Function for converting nav_stat in a dummy variable\n",
    "def convert_navstat(value):\n",
    "    if value in [0, 8]:\n",
    "        return 1  # MOVING\n",
    "    else:\n",
    "        return 0  # STOPPED"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u943EDWs81TM",
    "ExecuteTime": {
     "end_time": "2024-11-10T17:02:02.981815Z",
     "start_time": "2024-11-10T17:01:57.255616Z"
    }
   },
   "source": [
    "# Dataset loading\n",
    "data_train = pd.read_csv(\"ais_train.csv\", sep='|', header=0)\n",
    "data_test = pd.read_csv(\"ais_test.csv\", sep=',', header=0)\n",
    "schedule_dataset = pd.read_csv(\"schedules_to_may_2024.csv\", sep='|', header=0)\n",
    "ports_dataset = pd.read_csv(\"ports.csv\", sep='|', header=0)\n",
    "vessel_dataset = pd.read_csv(\"vessels.csv\", sep='|', header=0)\n",
    "\n",
    "# Convert the 'time' column in datetime format\n",
    "data_train['time'] = pd.to_datetime(data_train['time'])\n",
    "data_test['time'] = pd.to_datetime(data_test['time'])\n",
    "\n",
    "# Add the information about destination port to ais_train\n",
    "data_train_merged = pd.merge(data_train, ports_dataset, on='portId', how='left')\n",
    "data_train_merged.rename(columns={'latitude_y': 'latitudePort', 'longitude_y': 'longitudePort'}, inplace=True)\n",
    "# Drop of the unuseful columns\n",
    "data_train_merged.drop(columns=['name', 'portLocation', 'UN_LOCODE', 'countryName', 'ISO'], inplace=True)\n",
    "\n",
    "# Add the information about the vessel and drop useless columns\n",
    "train_preproc = pd.merge(data_train_merged, vessel_dataset, on='vesselId', how='left')\n",
    "train_preproc.dropna(subset=['portId'], inplace=True)\n",
    "train_preproc.drop(columns=['DWT','NT','vesselType','breadth','depth','draft','enginePower','freshWater','fuel','homePort','maxHeight','maxSpeed','maxWidth','rampCapacity','yearBuilt'],inplace=True)\n",
    "\n",
    "# Configuration and application of the IterativeImputer\n",
    "train_preproc['cog'] = train_preproc['cog'].replace(360, np.nan)\n",
    "imputer = IterativeImputer(max_iter=30, random_state=0)\n",
    "train_preproc[['cog']] = imputer.fit_transform(train_preproc[['cog']])"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T17:02:07.686831Z",
     "start_time": "2024-11-10T17:02:02.982881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DATA CLEANING\n",
    "train_preproc = train_preproc[(train_preproc['cog'] < 360.0) & (train_preproc['sog'] <= 102.2) & (train_preproc['rot'] >= -127) & (train_preproc['rot'] <= 127) & (train_preproc['heading'] <= 359) & (train_preproc['navstat'] >= 0) & (train_preproc['navstat'] <= 8)]\n",
    "train_preproc = train_preproc[(train_preproc['latitude_x'] <= 90) & (train_preproc['latitude_x'] >= -90) & (train_preproc['longitude_x'] <= 180) & (train_preproc['longitude_x'] >= -180)]\n",
    "\n",
    "# Trasformation of the 'nav_stat' column in a dummy variable (1 = moving, 0 = not moving)\n",
    "train_preproc['navstat_dummy'] = train_preproc['navstat'].apply(convert_navstat)\n",
    "\n",
    "# Summarize the three variables 'CEU', 'GT' and 'length' in a single feature which captures the maximum variance: 'vessel_dimensions'\n",
    "pca = PCA(n_components=1)\n",
    "train_preproc['vessel_dimensions'] = pca.fit_transform(train_preproc[[\"CEU\", \"GT\", \"length\"]])\n",
    "\n",
    "# Sorting the dataset in vessels and by time\n",
    "train_preproc = train_preproc.sort_values(by=['vesselId','time'])\n",
    "# Filtering the observations: we keep the vessels with more than 10 observations\n",
    "train_preproc = train_preproc.groupby('vesselId').filter(lambda x: len(x) >= 10)\n",
    "\n",
    "# Building a column with the time horizon for which the prediction is made (difference between the present timestamp and the following one)\n",
    "train_preproc['time_horizon'] = -train_preproc.groupby('vesselId')['time'].diff(-1)\n",
    "train_preproc['time_horizon'] = train_preproc['time_horizon'].dt.total_seconds()\n",
    "\n",
    "# 'latitude_future' and 'longitude_future' represent the next observation's position\n",
    "train_preproc['latitude_future'] = train_preproc.groupby('vesselId')['latitude_x'].shift(-1)\n",
    "train_preproc['longitude_future'] = train_preproc.groupby('vesselId')['longitude_x'].shift(-1)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MxVFxNkr81TO",
    "ExecuteTime": {
     "end_time": "2024-11-10T17:02:08.193920Z",
     "start_time": "2024-11-10T17:02:07.686831Z"
    }
   },
   "source": [
    "# PREPARATION OF THE TEST SET\n",
    "\n",
    "# Array with vesselId's for which a prediction is needed\n",
    "distinct_Id_tobepred = data_test['vesselId'].unique()\n",
    "\n",
    "# Train dataset filtered for those vessels\n",
    "data_vessels_tobepred = train_preproc[train_preproc['vesselId'].isin(distinct_Id_tobepred)]\n",
    "\n",
    "# Dataset containing the last observation for each vessel\n",
    "last_observations = data_vessels_tobepred.groupby('vesselId').tail(1)\n",
    "\n",
    "# Drop 'latitude_future' and 'longitude_future' from this dataset: they're all NaN\n",
    "last_observations_clean = last_observations.drop(columns=['latitude_future', 'longitude_future'])\n",
    "\n",
    "# Creation of the final dataset for test\n",
    "data_test_complete = pd.merge(last_observations_clean, data_test, on='vesselId', how='left')\n",
    "data_test_complete['time_horizon'] = data_test_complete['time_y']-data_test_complete['time_x']\n",
    "data_test_complete['time_horizon'] = data_test_complete['time_horizon'].dt.total_seconds()\n",
    "\n",
    "# Delete from the train the last row for each vessel\n",
    "train_preproc = train_preproc.dropna(subset=['time_horizon'])\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oG4s7GqL81TP",
    "outputId": "abe5af70-7fae-4367-f5c5-3b5766e28cfb",
    "ExecuteTime": {
     "end_time": "2024-11-10T17:16:38.484691Z",
     "start_time": "2024-11-10T17:02:08.194744Z"
    }
   },
   "source": [
    "# APPLICATION OF THE MODEL\n",
    "\n",
    "# Features and target selection\n",
    "features = ['latitude_x','longitude_x', 'cog', 'sog', 'heading', 'time_horizon','latitudePort','longitudePort', 'vessel_dimensions', 'rot']\n",
    "target = ['latitude_future','longitude_future']\n",
    "\n",
    "# Initialization of the scaler for normalizing the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Train set\n",
    "X = train_preproc[features]\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Target\n",
    "Y = train_preproc[target]\n",
    "\n",
    "# Definition of the model\n",
    "Model = RandomForestRegressor(n_estimators= 200, min_samples_split= 2, min_samples_leaf= 1, max_features= 'sqrt', max_depth= 20, bootstrap= True, random_state=11)\n",
    "\n",
    "# Fitting of the model\n",
    "Model.fit(X_scaled,Y)\n",
    "\n",
    "# Predictions\n",
    "X_test = data_test_complete[features]\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "Y_pred = Model.predict(X_test_scaled)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dg0n7VBa81TR",
    "ExecuteTime": {
     "end_time": "2024-11-10T17:16:38.862847Z",
     "start_time": "2024-11-10T17:16:38.504275Z"
    }
   },
   "source": [
    "# Saving of the prediction in the test dataset\n",
    "data_test_complete['latitude_predicted'] = Y_pred[:,0]\n",
    "data_test_complete['longitude_predicted'] = Y_pred[:,1]\n",
    "\n",
    "# Exporting the prediction in 'output_file'\n",
    "output = data_test_complete.sort_values(by='ID')\n",
    "output = output.reset_index(drop=True)\n",
    "output[['ID','longitude_predicted','latitude_predicted']].to_csv('output_file.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T17:16:39.052448Z",
     "start_time": "2024-11-10T17:16:38.864370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# POSTPROCESSING\n",
    "\n",
    "# Configuration for Pandas Display Options\n",
    "#pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Read the output file and the test set and merge them\n",
    "df = pd.read_csv('output_file.csv', sep=',')\n",
    "test = pd.read_csv('ais_test.csv', sep=',')\n",
    "test = test.drop(columns=['scaling_factor']) \n",
    "show = pd.merge(df, test, on='ID', how='left')\n",
    "show['time'] = pd.to_datetime(show['time'])"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T17:21:41.806080Z",
     "start_time": "2024-11-10T17:21:40.244351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the coordinates of lands and oceans from the files\n",
    "land_world = gpd.read_file('ne_10m_land/ne_10m_land.shp').to_crs(4326)\n",
    "ocean_world = gpd.read_file('ne_10m_ocean/ne_10m_ocean.shp').to_crs(4326)\n",
    "gdf = gpd.GeoDataFrame(show, geometry=gpd.points_from_xy(df['longitude_predicted'], df['latitude_predicted'], crs=\"EPSG:4326\"))"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T17:23:00.307683Z",
     "start_time": "2024-11-10T17:21:41.806080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Identify points on land\n",
    "points_on_land = gpd.sjoin(gdf, land_world, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "# Find the closest point in ocean\n",
    "closest_longitudes = []\n",
    "closest_latitudes = []\n",
    "for _, row in points_on_land.iterrows():\n",
    "    closest_point, _ = nearest_points(ocean_world['geometry'], row['geometry'])\n",
    "    closest_longitudes.append(closest_point.x)\n",
    "    closest_latitudes.append(closest_point.y)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T17:23:00.801015Z",
     "start_time": "2024-11-10T17:23:00.307683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Update predictions\n",
    "points_on_land['closest_longitude'] = closest_longitudes\n",
    "points_on_land['closest_latitude'] = closest_latitudes\n",
    "\n",
    "gdf.loc[points_on_land.index, 'longitude_predicted'] = points_on_land['closest_longitude']\n",
    "gdf.loc[points_on_land.index, 'latitude_predicted'] = points_on_land['closest_latitude']"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucaa\\AppData\\Local\\Temp\\ipykernel_28392\\2778354491.py:5: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0    120.290298\n",
      " dtype: float64  0    3.219418\n",
      "                 dtype: float64 0    18.659067\n",
      "                                dtype: float64 ... 0    11.4463\n",
      "                                                   dtype: float64\n",
      " 0   -97.774281\n",
      " dtype: float64 0    7.428355\n",
      "                dtype: float64]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  gdf.loc[points_on_land.index, 'longitude_predicted'] = points_on_land['closest_longitude']\n",
      "C:\\Users\\lucaa\\AppData\\Local\\Temp\\ipykernel_28392\\2778354491.py:6: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0    14.82023\n",
      " dtype: float64 0    51.348853\n",
      "                dtype: float64 0    54.41325\n",
      "                               dtype: float64 ... 0    53.906155\n",
      "                                                  dtype: float64\n",
      " 0    27.468492\n",
      " dtype: float64 0    53.693088\n",
      "                dtype: float64]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  gdf.loc[points_on_land.index, 'latitude_predicted'] = points_on_land['closest_latitude']\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T17:23:02.132707Z",
     "start_time": "2024-11-10T17:23:00.801015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Final processing and saving\n",
    "gdf['longitude_predicted'] = gdf['longitude_predicted'].astype(float)\n",
    "gdf['latitude_predicted'] = gdf['latitude_predicted'].astype(float)\n",
    "\n",
    "gdf = gdf.drop(columns=['vesselId', 'time', 'geometry'])\n",
    "gdf.to_csv('processed_res.csv',index=False)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucaa\\AppData\\Local\\Temp\\ipykernel_28392\\3723528813.py:2: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  gdf['longitude_predicted'] = gdf['longitude_predicted'].astype(float)\n",
      "C:\\Users\\lucaa\\AppData\\Local\\Temp\\ipykernel_28392\\3723528813.py:3: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  gdf['latitude_predicted'] = gdf['latitude_predicted'].astype(float)\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
